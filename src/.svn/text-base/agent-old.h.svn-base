/*
 * agent.h
 *
 *  Created on: 31 mei 2013
 *      Author: H. J. Meijdam
 */

#ifndef AGENT_H_
#define AGENT_H_

#include <deque>
#include <ros/ros.h>
#include <mprl_msgs/Action.h>
#include <mprl_msgs/StateReward.h>
#include <mprl_msgs/EnvDescription.h>
#include <mprl_common/config.h>
#include <mprl_common/mprl.h>
#include <mprl_common/communication.h>




using namespace std;
using namespace mprl;

class QTable
{
	protected:
	int qvalues_;
	//Array to store all Q values
	std::vector<Observation> check_states_;
	std::vector<Action> check_actions_;
	Vector Qtable_;
	//Number of states dimensions
	int dims_;
	//Number of states
	int numb_states_;
	//Number of action dimensions
	int action_dims_;
	//Number of actions
	int numb_actions_;

	//Maximum of each state
	Vector state_max_;
	//Minimum of each state
	Vector state_min_;
	//Maximum of each state
	Vector action_max_;
	//Minimum of each state
	Vector action_min_;
	//Stepsize of each state dimension
	Vector state_steps_;


	public:
	void checkStateAndAction(const Observation &state, const Action &action, int index);
	bool init( const int numb_states,const int actions, Vector &observation_max, Vector observation_min, Vector &action_max,Vector &action_min, float q_init_min, float q_init_max);
	float getQValue(const Observation &state,const Action& action);
	bool setQValue(const Observation &state,const Action& action, float QValue);
	int getQIndex(const Observation &state,const Action &action);
};


class MAgent : public BasicAgent
{
  protected:
	bool trace_;
	bool average_;
	int numb_actions_;
	int numb_states_;
	int average_over_;
	Action previous_action_;
	Observation previous_state_;
	std::deque<Action> previous_actions_;
	std::deque<Observation> previous_states_;
	std::deque<float> eligibility_;
	Vector action_min_;
	Vector action_max_;
	Vector action_steps_;
	Vector mean_q_value_;
	Vector mean_reward_;
	QTable Qtable_;
	float previousQValue_;
	double gamma_;
	double epsilon_;
	double alpha_;
	double lambda_;
	int trace_size_;
  public:
	MAgent(){};
    ~MAgent(){};
    double evaluate(const Observation &obs, ValueUpdaterPtr updater)
    {
    	cout<<"evaluate() has been called"<<endl;
    	return 0;
    }
    void set(const Observation &obs, const Action &action, double value)
    {
    	cout<<"set() has been called"<<endl;
    }

    void experience(const std::vector<mprl::Transition> &transitions)
    {
      cout<<"experience() has been called"<<endl;
    }

    void init(const Configuration &config)
    {
    	BasicAgent::init(config);
    	std::vector<double> config_actions_ =config["action_steps"];
    	Vector obs_min_=config["observation_min"];
    	Vector obs_max_=config["observation_max"];
    	action_min_=config["action_min"];
    	action_max_=config["action_max"];
    	config.get("state_steps", numb_states_, 19);
   		numb_actions_=config_actions_.at(0);
   		double q_init_min, q_init_max;
   		config.get("q_init_min", q_init_min, 0.0);
   		config.get("q_init_max", q_init_max, 0.1);
   		Qtable_.init(numb_states_,numb_actions_,obs_max_,obs_min_,action_max_, action_min_,q_init_min, q_init_max);
   		config.get("gamma", gamma_, 0.99);
   		config.get("alpha", alpha_, 0.2);
   		config.get("epsilon", epsilon_, 0.05);
   		config.get("lambda", lambda_, 0.9);
   		config.get("average_over", average_over_, 20);
   		trace_=config.has("trace_size");
   		config.get("trace_size", trace_size_, 20);
   		average_=config.has("average_over");
   		cout<<"gamma: "<<gamma_<<endl;
   		cout<<"alpha: "<<alpha_<<endl;
   		cout<<"epsilon: "<<epsilon_<<endl;
   		//once the env_description is published the Qtable can be initialised
   		action_steps_.resize(action_max_.size());
   		for(int i=0; i<action_max_.size();i++)
   		{
   			action_steps_.at(i)=(action_max_.at(i)-action_min_.at(i))/(numb_actions_-1);
   		}
    }
    bool start(const Observation &obs, Action &action)
    {
         BasicAgent::start(obs, action);
         action=getAction(obs);
         previous_action_=action;
         previous_state_=obs;
         previousQValue_=Qtable_.getQValue(previous_state_, previous_action_);
         previous_states_.clear();
         previous_actions_.clear();
         eligibility_.clear();
         previous_actions_.push_front(action);
         previous_states_.push_front(obs);
         eligibility_.push_front(1);
         return true;
    }
    bool step(double reward, const Observation &observation, Action &action)
    {
      	BasicAgent::step(reward, observation, action);
      	if((float) rand() / (RAND_MAX)>epsilon_|| isTestRun())
      	{
      		action=getAction(observation);
      	}
      	else
      	{
      		action=randomAction();
      		previous_states_.resize(1);
      		previous_actions_.resize(1);
      		eligibility_.resize(1);
      	}
      	if(trace_)
      	{
      		updateTrace(observation, action, reward);
      		checkTrace();
      	}
      	else
      	{
      		updateSimple(observation, action, reward);
      	}
      	previous_action_=action;
      	previous_state_=observation;
      	return true;
    }
    void end(double reward)
    {
      	BasicAgent::end(reward);
      	if(trace_)
       	{
      		updateAbsorbingTrace(reward);
       	}
       	else
      	{
       		updateTerminalAndAbsorbing(reward);
       	}
       	previous_states_.clear();
       	previous_actions_.clear();
       	eligibility_.clear();
    }
    bool updateSimple(const Observation &state,const Action &action, float reward)
    {
    	float tempQ=Qtable_.getQValue(previous_state_, previous_action_);
    	float delta=reward+gamma_*Qtable_.getQValue(state, action)-tempQ;
    	if(Qtable_.setQValue(previous_state_, previous_action_, tempQ+alpha_*delta)){};
    	previous_state_=state;
    	previous_action_=action;
    	return true;
    }
    void averageOver(float& previousQValue_, float& reward)
    {
     	mean_q_value_.push_back(previousQValue_);
       	mean_reward_.push_back(reward);
      	if(mean_q_value_.size()>=average_over_)
       	{
       		int total=0;
       		for(int i=0; i<mean_q_value_.size();i++)
       		{
       			total=total+mean_q_value_.at(i);
       		}
       		cout<<"Average Q value of "<<mean_q_value_.size()<<" steps: "<<(float)total/(float)mean_q_value_.size();
       		mean_q_value_.clear();
       		total=0;
       		for(int i=0; i<mean_reward_.size();i++)
       		{
       			total=total+mean_reward_.at(i);
       		}

       		cout<<" Average reward of "<<mean_reward_.size()<<" steps: "<<(float)total/(float)mean_reward_.size();
        	cout<<" Predicted average Q value: "<<(float)total/(float)mean_reward_.size()*(1/(1-gamma_))<<endl;
       		mean_reward_.clear();
       	}
    }
    bool updateTerminalAndAbsorbing(float reward)
    {
       	float previousQValue=Qtable_.getQValue(previous_state_, previous_action_);
       	//cout<<"state.at(0), state.at(1), action.at(0):"<<state.at(0)<<", "<< state.at(1)<<" "<<action.at(0)<<endl;
       	//cout<<"previousQValue: "<<previousQValue<<endl;
       	float delta=reward-previousQValue;
       	//cout<<"delta: "<<delta<<endl;
       	Qtable_.setQValue(previous_state_, previous_action_, previousQValue+alpha_*delta);
       	//cout<<"Qtable_.getQValue(previous_state_, previous_action_): "<<Qtable_.getQValue(previous_state_, previous_action_)<<endl;
       	return true;
    }
    Action getAction(const Observation &state)
    {

    	Action action=action_min_;
    	Action temp_action=action_min_;
    	float high=Qtable_.getQValue(state, temp_action);
    	//cout<<"high: "<<high<<endl;
    	while(temp_action!=action_max_)
    	{
    		for(int i=0;i<action_min_.size();i++)
    		{
    			for(int k=0;k<action_min_.size();k++)
    			{
    				temp_action.at(k)=temp_action.at(k)+action_steps_.at(k);
    				float temp_val=Qtable_.getQValue(state, temp_action);
    				//cout<<"temp_val: "<<temp_val<<endl;
    				//cout<< "temp_action.at(0): "<<temp_action.at(0)<<endl;
    				if(temp_val>high)
    				{
    					high=temp_val;
    					action=temp_action;
    				}

    			}
    		}
    	}
    	//cout<<"action.at(0): "<<action.at(0)<<endl;
    	return action;
    }
    Action randomAction()
    {
    	Action action;
    	action.resize(action_min_.size());
    	for(int i=0; i<action_min_.size(); i++)
    	{
    		action.at(i)=action_min_.at(i)+round( ((float)rand() / (RAND_MAX))*(numb_actions_-1))*action_steps_.at(i);
    		//cout<<"env_action.action.at(i): "<<env_action.action.at(i)<<endl;
    	}
    	//cout<<"action.at(0): "<<action.at(0)<<endl;
    	return action;
    }
    void cleanup()
    {
    	BasicAgent::cleanup();
    }
    void checkTrace()
    {
    	if(previous_states_.size()!=previous_actions_.size())
    	{
    		cout<<"Trace corrupted, previous states and actions do not match"<<endl;
    		exit(0);
    	}
    	if(previous_states_.size()!=eligibility_.size())
    	{
    		cout<<"Trace corrupted, previous states and eligibility do not match"<<endl;
    		exit(0);
    	}
    }
    void message(const Configuration &message, Configuration &result)
    {
    }
    bool updateTrace(const Observation &state, const Action &action, float reward)
    {
    	//cout<<"current Qvalue index: ";
    	float previousQValue=Qtable_.getQValue(previous_states_.at(0), previous_actions_.at(0));
    	//cout<<"reward: "<< reward<<endl;
    	//cout<<"Qtable_.getQValue(state, action): "<<Qtable_.getQValue(state, action)<<endl;
    	float delta=reward+gamma_*Qtable_.getQValue(state, action)-previousQValue;
    	//cout<<"delta: "<<delta<<endl;
    	for(int i=1; i<previous_states_.size(); i++)
        {
    		if(previous_states_.at(i)==previous_states_.at(0) && previous_actions_.at(i)==previous_actions_.at(0))
    		{
    			previous_states_.erase(previous_states_.begin()+i);
				previous_actions_.erase(previous_actions_.begin()+i);
				eligibility_.erase(eligibility_.begin()+i);
    		}
        }

    	if(previous_states_.size()>trace_size_)
    	{
    	   	previous_states_.resize(trace_size_);
    		previous_actions_.resize(trace_size_);
    		eligibility_.resize(trace_size_);
    	}

    	for(int i=0; i<previous_states_.size(); i++)
    	{
    		//cout<<"trace index get value: ";
    		float previousQValue=Qtable_.getQValue(previous_states_.at(i), previous_actions_.at(i));
    		//cout<<"trace index set value: ";
    		if(Qtable_.setQValue(previous_states_.at(i), previous_actions_.at(i), previousQValue+alpha_*delta*eligibility_.at(i))){};
    		//cout<<"alpha_*delta*eligibility_.at(i): "<<alpha_*delta*eligibility_.at(i)<<endl;
    		//cout<<"eligibility_.at(i): "<<eligibility_.at(i)<<" i: "<< i<<endl;
    		//cout<<"delta: "<<delta<<endl;
    		//cout<<"alpha_: "<<alpha_<<endl;
    		eligibility_.at(i)=eligibility_.at(i)*gamma_*lambda_;

    	}
    	previous_states_.push_front(state);
    	previous_actions_.push_front(action);
    	eligibility_.push_front(1);
    	if(previous_states_.at(0)!=state)
    	{
    		cout<<"Wrong way of building previous_states_ vector"<<endl;
    		exit(0);
    	}
    	if(average_)
    	{
    		averageOver(previousQValue,reward);
    	}
    	return true;

    }
    bool updateAbsorbingTrace(float reward)
    {
       	//SARSA update
       	//cout<<"previousQvalue index: ";
    	float previousQValue=Qtable_.getQValue(previous_states_.at(0), previous_actions_.at(0));
       	//cout<<"current Qvalue index: ";
       	float delta=reward-previousQValue;
       	//cout<<"delta: "<<delta<<endl;
       	for(int i=1; i<previous_states_.size(); i++)
       	{
       		if(previous_states_.at(i)==previous_states_.at(0) && previous_actions_.at(i)==previous_actions_.at(0))
       		{
       			previous_states_.erase(previous_states_.begin()+i);
       			previous_actions_.erase(previous_actions_.begin()+i);
       			eligibility_.erase(eligibility_.begin()+i);
       		}
       	}
       	if(previous_states_.size()>trace_size_)
       	{
       	   	previous_states_.resize(trace_size_);
       		previous_actions_.resize(trace_size_);
       	}
       	for(int i=0; i<previous_states_.size(); i++)
       	{
       		//cout<<"trace index get value: ";
       		float previousQValue=Qtable_.getQValue(previous_states_.at(i), previous_actions_.at(i));
       		//cout<<"trace index set value: ";
       		if(Qtable_.setQValue(previous_states_.at(i), previous_actions_.at(i), previousQValue+alpha_*delta*eligibility_.at(i))){};
       		//cout<<"eligibility_.at(i): "<<eligibility_.at(i)<<"i: "<< i<<endl;
       	}
       	previous_states_.clear();
       	previous_actions_.clear();
       	eligibility_.clear();
       	return true;
    }
};






#endif /* AGENT_H_ */
